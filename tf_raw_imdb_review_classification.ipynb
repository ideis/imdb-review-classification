{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ideis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 25000, Test: 25000, Unlabeled train: 50000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv( \"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print(\"Train: %d, Test: %d, Unlabeled train: %d \\n\"\n",
    "      % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(text, remove_stopwords=False):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [review_to_wordlist(s) for s in raw_sentences if len(raw_sentences) > 0]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "for review in test[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = \"word2vec_300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrible', 0.7701743841171265), ('atrocious', 0.746411144733429), ('abysmal', 0.7401413917541504), ('dreadful', 0.7226380109786987), ('horrible', 0.7210754156112671), ('appalling', 0.6851282119750977), ('horrid', 0.6848453283309937), ('horrendous', 0.6756206154823303), ('lousy', 0.6487287282943726), ('bad', 0.5936079025268555)]\n",
      "\n",
      "[('amazing', 0.7523432374000549), ('incredible', 0.6692808866500854), ('fantastic', 0.6562114953994751), ('excellent', 0.6531256437301636), ('outstanding', 0.6119468212127686), ('exceptional', 0.6035537719726562), ('great', 0.5909483432769775), ('cool', 0.5837295651435852), ('terrific', 0.5589351654052734), ('astounding', 0.5587907433509827)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"awful\"))\n",
    "print()\n",
    "print(model.most_similar(\"awesome\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(words, model, num_features):\n",
    "   \n",
    "    features = np.zeros((num_features,),dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0.\n",
    "    vocab = set(model.wv.vocab)\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab: \n",
    "            nwords = nwords + 1.\n",
    "            features = np.add(features,model[word])\n",
    "\n",
    "    features = np.divide(features,nwords)\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_review_features(reviews, model, num_features):\n",
    "\n",
    "    counter = 0\n",
    "    review_features = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        review_features[counter] = make_features(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return review_features\n",
    "\n",
    "\n",
    "clean_train_reviews = [review_to_wordlist(review, remove_stopwords=True) for review in train['review']]\n",
    "X_train = get_review_features(clean_train_reviews, model, num_features)\n",
    "\n",
    "\n",
    "clean_test_reviews = [review_to_wordlist(review, remove_stopwords=True) for review in test['review']]\n",
    "X_test = get_review_features(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000, 2)\n",
      "(25000, 300)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def extract_sentiment(s):\n",
    "    s = re.sub(r\"[^0-9]\", \" \", s)\n",
    "    n = int(s.split()[1])\n",
    "    if n <= 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "test['sentiment'] = test.apply(lambda x: extract_sentiment(x['id']), axis=1)\n",
    "\n",
    "y_train = to_categorical(train['sentiment'], num_classes=2)\n",
    "y_test = to_categorical(test['sentiment'], num_classes=2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 50\n",
    "display_epoch = False\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 32\n",
    "num_input = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(X):\n",
    "    h1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "    a1 = tf.nn.relu(h1, name='a1')\n",
    "#     a1 = tf.nn.dropout(a1, 0.2) \n",
    "\n",
    "    h2 = tf.add(tf.matmul(a1, weights['w2']), biases['b2'])\n",
    "    a2 = tf.nn.relu(h2, name='a2')\n",
    "#     a2 = tf.nn.dropout(a2, 0.2) \n",
    "\n",
    "    h3 = tf.add(tf.matmul(a2, weights['w3']), biases['b3'])\n",
    "    a3 = tf.nn.relu(h3, name='a3')\n",
    "    \n",
    "    out_layer = tf.matmul(a3, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "\n",
      "Testing Accuracy: 0.85012\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for step in range(1, epochs+1):\n",
    "        # Run optimization(backprop)\n",
    "        sess.run(train_op, feed_dict={X: X_train, Y: y_train})\n",
    "        if display_epoch:\n",
    "            # Calculate loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_train,\n",
    "                                                                 Y: y_train})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print()\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test,\n",
    "                                      Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(train, y, folds):\n",
    "    fold_size = int(train.shape[0] / folds)\n",
    "\n",
    "    train_val_folds = list()\n",
    "    train_y_folds = list()\n",
    "    \n",
    "    test_val_folds = list()\n",
    "    test_y_folds = list()\n",
    "    \n",
    "    \n",
    "    for k in range(1, folds+1):\n",
    "        train_val_fold = np.concatenate((train[:fold_size*(k-1)], train[fold_size*k:]), axis=0)\n",
    "        train_y_fold = np.concatenate((y[:fold_size*(k-1)], y[fold_size*k:]), axis=0)\n",
    "        train_val_folds.append(train_val_fold)\n",
    "        train_y_folds.append(train_y_fold)\n",
    "        \n",
    "        test_val_fold = train[((k-1)*fold_size):(k*fold_size)]\n",
    "        test_y_fold = y[((k-1)*fold_size):(k*fold_size)]\n",
    "        test_val_folds.append(test_val_fold)\n",
    "        test_y_folds.append(test_y_fold)\n",
    "        \n",
    "    return (train_val_folds, train_y_folds, test_val_folds, test_y_folds)\n",
    "\n",
    "folds = 5\n",
    "train_val_folds, train_y_folds, test_val_folds, test_y_folds = cross_validation_split(X_train, y_train, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Loss= 0.8905, Accuracy= 0.834\n",
      "Fold 1, Loss= 0.7587, Accuracy= 0.818\n",
      "Fold 2, Loss= 0.4992, Accuracy= 0.840\n",
      "Fold 3, Loss= 0.5436, Accuracy= 0.802\n",
      "Fold 4, Loss= 0.3430, Accuracy= 0.874\n",
      "Mean Accuracy after 5 folds: 0.834\n",
      "Testing Accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for k in range(folds):\n",
    "        for step in range(1, epochs+1):\n",
    "            sess.run(train_op, feed_dict={X: train_val_folds[k], Y: train_y_folds[k]})\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: test_val_folds[k],Y: test_y_folds[k]})\n",
    "        total_accuracy += acc\n",
    "        print(\"Fold \" + str(k) + \", Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Mean Accuracy after \" + str(folds) +\" folds: \" + \"{:.3f}\".format(total_accuracy/folds))\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: X_test, Y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
