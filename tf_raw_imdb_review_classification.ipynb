{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ideis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"russian\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "data['review'] = data.apply(lambda x: clean_text(x['review']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set:\n",
      "  X_train: 20000\n",
      "  y_train: 20000\n",
      "\n",
      "Testing set:\n",
      "  X_test: 5000\n",
      "  y_test: 5000\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, labels, train_test_split=0.8):\n",
    "    data_size = len(data)\n",
    "    test_size = int(data_size - round(data_size * train_test_split))\n",
    "    \n",
    "    print(\"\\nTraining set:\")\n",
    "    X_train = data[test_size:]\n",
    "    print(\"  X_train: {}\".format(len(X_train)))\n",
    "    y_train = labels[test_size:]\n",
    "    print(\"  y_train: {}\".format(len(y_train)))\n",
    "    \n",
    "    print(\"\\nTesting set:\")\n",
    "    X_test = data[:test_size]\n",
    "    print(\"  X_test: {}\".format(len(X_test)))\n",
    "    y_test = labels[:test_size]\n",
    "    print(\"  y_test: {}\".format(len(y_test)))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(data, data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73272 words in a dictionary\n"
     ]
    }
   ],
   "source": [
    "num_words = 1000\n",
    "\n",
    "all_reviews = data['review']\n",
    "all_reviews.head()\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "total_words = len(tokenizer.word_index)\n",
    "print('{} words in a dictionary'.format(total_words))\n",
    "\n",
    "X_train = tokenizer.texts_to_matrix(X_train['review'])\n",
    "X_test = tokenizer.texts_to_matrix(X_test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2\n",
      "(5000, 1000)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape[1])\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 25\n",
    "display_epoch = True\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 32\n",
    "num_input = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(X):\n",
    "    h1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "    a1 = tf.nn.relu(h1, name='a1')\n",
    "#     a1 = tf.nn.dropout(a1, 0.2) \n",
    "\n",
    "    h2 = tf.add(tf.matmul(a1, weights['w2']), biases['b2'])\n",
    "    a2 = tf.nn.relu(h2, name='a2')\n",
    "#     a2 = tf.nn.dropout(a2, 0.2) \n",
    "\n",
    "    h3 = tf.add(tf.matmul(a2, weights['w3']), biases['b3'])\n",
    "    a3 = tf.nn.relu(h3, name='a3')\n",
    "    \n",
    "    out_layer = tf.matmul(a3, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= 897.7988, Accuracy= 0.505\n",
      "Step 2, Loss= 265.3820, Accuracy= 0.505\n",
      "Step 3, Loss= 286.9371, Accuracy= 0.534\n",
      "Step 4, Loss= 119.0917, Accuracy= 0.577\n",
      "Step 5, Loss= 182.0977, Accuracy= 0.590\n",
      "Step 6, Loss= 122.3391, Accuracy= 0.606\n",
      "Step 7, Loss= 67.2420, Accuracy= 0.619\n",
      "Step 8, Loss= 96.3561, Accuracy= 0.625\n",
      "Step 9, Loss= 81.1545, Accuracy= 0.628\n",
      "Step 10, Loss= 48.3432, Accuracy= 0.639\n",
      "Step 11, Loss= 71.3577, Accuracy= 0.654\n",
      "Step 12, Loss= 59.4069, Accuracy= 0.665\n",
      "Step 13, Loss= 34.0796, Accuracy= 0.678\n",
      "Step 14, Loss= 38.9757, Accuracy= 0.692\n",
      "Step 15, Loss= 39.3344, Accuracy= 0.702\n",
      "Step 16, Loss= 26.6386, Accuracy= 0.718\n",
      "Step 17, Loss= 22.6015, Accuracy= 0.726\n",
      "Step 18, Loss= 27.0128, Accuracy= 0.735\n",
      "Step 19, Loss= 25.1094, Accuracy= 0.744\n",
      "Step 20, Loss= 18.9809, Accuracy= 0.743\n",
      "Step 21, Loss= 16.5664, Accuracy= 0.740\n",
      "Step 22, Loss= 17.1752, Accuracy= 0.746\n",
      "Step 23, Loss= 16.7863, Accuracy= 0.757\n",
      "Step 24, Loss= 15.0763, Accuracy= 0.759\n",
      "Step 25, Loss= 13.4477, Accuracy= 0.752\n",
      "Optimization Finished!\n",
      "\n",
      "Testing Accuracy: 0.7384\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, epochs+1):\n",
    "        # Run optimization(backprop)\n",
    "        sess.run(train_op, feed_dict={X: X_train, Y: y_train})\n",
    "        if display_epoch:\n",
    "            # Calculate loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_train,\n",
    "                                                                 Y: y_train})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print()\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test,\n",
    "                                      Y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
