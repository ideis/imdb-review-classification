{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ideis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "# data['review'] = [BeautifulSoup(text).get_text() for text in data['review']]\n",
    "# data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "data['review'] = data.apply(lambda x: clean_text(x['review']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, train_test_split=0.8):\n",
    "    data_size = len(data)\n",
    "    test_size = int(data_size - round(data_size * train_test_split))\n",
    "    \n",
    "    print(\"\\nTraining set:\")\n",
    "    X_train = data[test_size:]\n",
    "    print(\"  X_train: {}\".format(len(X_train)))\n",
    "    y_train = labels[test_size:]\n",
    "    print(\"  y_train: {}\".format(len(y_train)))\n",
    "    \n",
    "    print(\"\\nTesting set:\")\n",
    "    X_test = data[:test_size]\n",
    "    print(\"  X_test: {}\".format(len(X_test)))\n",
    "    y_test = labels[:test_size]\n",
    "    print(\"  y_test: {}\".format(len(y_test)))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(data, data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "\n",
    "all_reviews = data['review']\n",
    "all_reviews.head()\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "total_words = len(tokenizer.word_index)\n",
    "print('{} words in a dictionary'.format(total_words))\n",
    "\n",
    "X_train = tokenizer.texts_to_matrix(X_train['review'])\n",
    "X_test = tokenizer.texts_to_matrix(X_test['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 25\n",
    "display_epoch = True\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 32\n",
    "num_input = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(X):\n",
    "    h1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "    a1 = tf.nn.relu(h1, name='a1')\n",
    "#     a1 = tf.nn.dropout(a1, 0.2) \n",
    "\n",
    "    h2 = tf.add(tf.matmul(a1, weights['w2']), biases['b2'])\n",
    "    a2 = tf.nn.relu(h2, name='a2')\n",
    "#     a2 = tf.nn.dropout(a2, 0.2) \n",
    "\n",
    "    h3 = tf.add(tf.matmul(a2, weights['w3']), biases['b3'])\n",
    "    a3 = tf.nn.relu(h3, name='a3')\n",
    "    \n",
    "    out_layer = tf.matmul(a3, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, epochs+1):\n",
    "        # Run optimization(backprop)\n",
    "        sess.run(train_op, feed_dict={X: X_train, Y: y_train})\n",
    "        if display_epoch:\n",
    "            # Calculate loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_train,\n",
    "                                                                 Y: y_train})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print()\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test,\n",
    "                                      Y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
